{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRP:\n",
    "    \n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "        self.assignement_list = []  # there are assignement_list[i] customers at table i\n",
    "        self.n_customers = 0\n",
    "        self.n_tables = 0\n",
    "    \n",
    "    def sample(self):\n",
    "        if not self.assignement_list:  # first customer to table 0\n",
    "            self.assignement_list.append(1)\n",
    "            self.n_customers = 1\n",
    "            self.n_tables = 1\n",
    "            return 0\n",
    "        else:\n",
    "            aux_assignement_list = self.assignement_list.copy()\n",
    "            aux_assignement_list.append(self.alpha)\n",
    "            prob_vec = np.array(aux_assignement_list, dtype='float32') / (self.n_customers + self.alpha)\n",
    "            table = np.random.choice(self.n_tables + 1, p=prob_vec)\n",
    "            self.n_customers += 1\n",
    "            if table == self.n_tables:  # new table is formed with prob prop to alpha\n",
    "                self.n_tables += 1\n",
    "                self.assignement_list.append(1)\n",
    "            else:  # otherwise customer sits at existing table with prob prop to its number of customers\n",
    "                self.assignement_list[table] += 1\n",
    "            return table\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_option(crp, policies, policy_prior, termination_distrs,\n",
    "                  termination_distr_prior, dim_states, dim_actions):\n",
    "    out = crp.sample()\n",
    "    if len(policies) < crp.n_tables:\n",
    "        aux_policy_list = []\n",
    "        aux_termination_distr_list = []\n",
    "        for s in range(dim_states):\n",
    "            policy = scipy.stats.rv_discrete(values=(range(dim_actions), policy_prior.rvs()[0]))\n",
    "            aux_policy_list.append(policy)\n",
    "            termination_distr = scipy.stats.bernoulli(p=termination_distr_prior.rvs())\n",
    "            aux_termination_distr_list.append(termination_distr)\n",
    "        policies.append(aux_policy_list)\n",
    "        termination_distrs.append(aux_termination_distr_list)\n",
    "    return out\n",
    "\n",
    "def generate_trajectories(N, T, dim_states, dim_actions, alpha):\n",
    "    # TODO: give seed as input for replicability\n",
    "    states = np.zeros([N, T], dtype='int64')\n",
    "    actions = np.zeros([N, T], dtype='int64')\n",
    "    options = np.zeros([N, T], dtype='int64')\n",
    "    terminations = np.zeros([N, T], dtype='bool8')\n",
    "    crp = CRP(alpha)\n",
    "    s0_prior = scipy.stats.rv_discrete(values=(range(dim_states), np.ones(dim_states) / dim_states))\n",
    "    policy_prior = scipy.stats.dirichlet(np.ones(dim_actions))  # same prior is used for all options and states\n",
    "    termination_distr_prior = scipy.stats.uniform()\n",
    "    policies = []\n",
    "    termination_distrs = []\n",
    "    transitions = []\n",
    "    for s in range(dim_states):\n",
    "        transitions_s = []\n",
    "        for a in range(dim_actions):\n",
    "            transition_probs = scipy.stats.dirichlet(np.ones(dim_states)).rvs()\n",
    "            transitions_s.append(scipy.stats.rv_discrete(values=(range(dim_states), transition_probs[0])))\n",
    "        transitions.append(transitions_s)\n",
    "    for i in range(N):\n",
    "        states[i, 0] = s0_prior.rvs()\n",
    "        terminations[i, 0] = 1\n",
    "        options[i, 0] = sample_option(crp, policies, policy_prior, termination_distrs,\n",
    "                                      termination_distr_prior, dim_states, dim_actions)\n",
    "        for t in range(T-1):\n",
    "            actions[i, t] = policies[options[i, t]][states[i, t]].rvs()\n",
    "            states[i, t+1] = transitions[states[i, t]][actions[i, t]].rvs()\n",
    "            terminations[i, t+1] = termination_distrs[options[i, t]][states[i, t+1]].rvs()\n",
    "            if terminations[i, t+1] == 0:\n",
    "                options[i, t+1] = options[i, t]\n",
    "            else:\n",
    "                options[i, t+1] = sample_option(crp, policies, policy_prior, termination_distrs,\n",
    "                                                termination_distr_prior, dim_states, dim_actions)\n",
    "        actions[i, -1] = policies[options[i, -1]][states[i, -1]].rvs()\n",
    "    \n",
    "    return states, actions, options, terminations, crp.n_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallVarianceOptimizer:\n",
    "    \n",
    "    def __init__(self, states, dim_states, actions, dim_actions, K, lambda_vec, steps,\n",
    "                 lr=0.1, device='cuda', clip=5.):\n",
    "        self.device = device\n",
    "        self.states = torch.tensor(states).to(self.device)  # [N, T]\n",
    "        self.actions = torch.tensor(actions).to(self.device)  # [N, T]\n",
    "        self.K = K\n",
    "        self.lambda_vec = lambda_vec\n",
    "        self.lr = lr\n",
    "        self.N, self.T = states.shape\n",
    "        self.dim_actions = dim_actions\n",
    "        self.steps = steps\n",
    "        self.relaxed_options_logits = torch.randn((N, T, K), requires_grad=True, device=self.device)\n",
    "        self.relaxed_terminations_logits = torch.randn((N, T), requires_grad=True, device=self.device)\n",
    "        self.termination_fn_logits = torch.randn((K, dim_states), requires_grad=True, device=self.device)\n",
    "        self.policies_logits = torch.randn((dim_states, K, dim_actions), requires_grad=True, device=self.device)\n",
    "        self.parameter_list = [self.relaxed_options_logits, self.relaxed_terminations_logits,\n",
    "                               self.termination_fn_logits, self.policies_logits]\n",
    "        self.optimizer = torch.optim.Adam(self.parameter_list, lr=lr, weight_decay=0.1*lr)\n",
    "        self.clip = clip\n",
    "        \n",
    "    def compute_objective(self, discretize=False):\n",
    "        relaxed_terminations = torch.sigmoid(self.relaxed_terminations_logits)\n",
    "        relaxed_options = nn.functional.softmax(self.relaxed_options_logits, dim=-1)\n",
    "        if discretize:  # in this case relaxed_terminations and relaxed_options are not relaxed\n",
    "            relaxed_terminations = torch.sign(relaxed_terminations - 0.5) * 0.5 + 0.5\n",
    "            relaxed_options = nn.functional.one_hot(torch.argmax(relaxed_options, dim=-1), self.K)\n",
    "        termination_fn = torch.sigmoid(self.termination_fn_logits)\n",
    "        policies = nn.functional.softmax(self.policies_logits, dim=2)\n",
    "        term1 = -self.lambda_vec[0] * self.K\n",
    "        flat_states = torch.reshape(self.states[:, 1:], (-1,))\n",
    "        flat_term_fns_trajs = torch.index_select(input=termination_fn, dim=1, index=flat_states)\n",
    "        term_fns_trajs = torch.transpose(torch.transpose(torch.reshape(\n",
    "            flat_term_fns_trajs, (self.K, self.N, self.T-1)), 0, 2), 0, 1)  # [N,T-1,K]\n",
    "        relaxed_term_probs = torch.sum(relaxed_options[:, :-1] * term_fns_trajs, dim=2)  # [N, T-1]\n",
    "#         relaxed_bce = nn.functional.binary_cross_entropy(input=relaxed_term_probs,\n",
    "#                                                          target=relaxed_terminations[:, 1:], reduction='none')\n",
    "        relaxed_bce = -relaxed_terminations[:, 1:] * torch.log(relaxed_term_probs) + (relaxed_terminations[:, 1:] - 1.) * torch.log(1. - relaxed_term_probs)\n",
    "        term2 = -self.lambda_vec[1] * (relaxed_bce +\n",
    "                                   torch.log(torch.maximum(relaxed_term_probs, 1. - relaxed_term_probs)))\n",
    "        flat_states_all = torch.reshape(self.states, (-1,))\n",
    "        flat_policies_traj_states = torch.index_select(input=policies, dim=0, index=flat_states_all)\n",
    "        policies_traj_states = torch.reshape(flat_policies_traj_states,\n",
    "                                             (self.N, self.T, self.K, self.dim_actions))\n",
    "        relaxed_policy_vals = torch.sum(torch.unsqueeze(relaxed_options, 3) * policies_traj_states, dim=2)\n",
    "        # relaxed_policy vals has shape [N, T, dim_actions]\n",
    "        relaxed_policy_at_actions = torch.squeeze(torch.gather(\n",
    "            input=relaxed_policy_vals, dim=2, index=torch.unsqueeze(self.actions, 2)))  # [N, T]\n",
    "        term4 = relaxed_policy_at_actions - torch.max(relaxed_policy_vals, dim=2)[0]\n",
    "        objective = term1 + torch.sum(term2) + torch.sum(term4)\n",
    "        if not discretize:\n",
    "            term3 = -(1. - relaxed_terminations[:, 1:]) * torch.linalg.norm(\n",
    "                relaxed_options[:, 1:] - relaxed_options[:, :-1], ord=2, dim=2)\n",
    "            term5 = relaxed_terminations * torch.log(relaxed_terminations) + (1. - relaxed_terminations) * torch.log(1. - relaxed_terminations)\n",
    "            term5 += torch.sum(relaxed_options * torch.log(relaxed_options), dim=2)\n",
    "            objective += self.lambda_vec[2] * torch.sum(term3) + self.lambda_vec[3] * torch.sum(term5)\n",
    "        return -objective / (self.N * self.T)\n",
    "    \n",
    "    def _gradient_step(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        negative_objective = self.compute_objective()\n",
    "        negative_objective.backward()\n",
    "        nn.utils.clip_grad_norm_(self.parameter_list, self.clip)\n",
    "        self.optimizer.step()\n",
    "        return negative_objective.detach().cpu().numpy()\n",
    "    \n",
    "    def train(self, verbose=True):\n",
    "        for i in range(self.steps):\n",
    "            negative_objective_np = self._gradient_step()\n",
    "            if verbose:\n",
    "                print(f'Finished epoch {i}\\twith loss: {negative_objective_np:f}\\t')\n",
    "        discrete_negative_objective = self.compute_objective(discretize=True)\n",
    "        print(f'Finished training with discrete loss: {discrete_negative_objective:f}\\t')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "N = 100  # number of trajectories\n",
    "T = 4  # number of steps per trajectory (corresponds to T-1 in the manuscript, last state is not sampled)\n",
    "dim_states = 3  # discrete state space, this is the number of states\n",
    "dim_actions = 2  # discrete action space, this is the number of actions\n",
    "alpha = 1.\n",
    "states, actions, options, terminations, true_K = generate_trajectories(N, T, dim_states, dim_actions, alpha)\n",
    "print(true_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0\twith loss: 19.176922\t\n",
      "Finished epoch 1\twith loss: 18.681585\t\n",
      "Finished epoch 2\twith loss: 18.245411\t\n",
      "Finished epoch 3\twith loss: 17.908915\t\n",
      "Finished epoch 4\twith loss: 17.712057\t\n",
      "Finished epoch 5\twith loss: 17.587275\t\n",
      "Finished epoch 6\twith loss: 17.480705\t\n",
      "Finished epoch 7\twith loss: 17.467039\t\n",
      "Finished epoch 8\twith loss: 17.468140\t\n",
      "Finished epoch 9\twith loss: 17.508423\t\n",
      "Finished epoch 10\twith loss: 17.530903\t\n",
      "Finished epoch 11\twith loss: 17.592415\t\n",
      "Finished epoch 12\twith loss: 17.572828\t\n",
      "Finished epoch 13\twith loss: 17.567417\t\n",
      "Finished epoch 14\twith loss: 17.642328\t\n",
      "Finished epoch 15\twith loss: 17.695250\t\n",
      "Finished epoch 16\twith loss: 17.720999\t\n",
      "Finished epoch 17\twith loss: 17.736547\t\n",
      "Finished epoch 18\twith loss: 17.726431\t\n",
      "Finished epoch 19\twith loss: 17.767076\t\n",
      "Finished epoch 20\twith loss: 17.812025\t\n",
      "Finished epoch 21\twith loss: 17.801205\t\n",
      "Finished epoch 22\twith loss: 17.801735\t\n",
      "Finished epoch 23\twith loss: 17.806765\t\n",
      "Finished epoch 24\twith loss: 17.868113\t\n",
      "Finished epoch 25\twith loss: 17.872753\t\n",
      "Finished epoch 26\twith loss: 17.835466\t\n",
      "Finished epoch 27\twith loss: 17.908302\t\n",
      "Finished epoch 28\twith loss: 17.924755\t\n",
      "Finished epoch 29\twith loss: 17.925905\t\n",
      "Finished epoch 30\twith loss: 17.883730\t\n",
      "Finished epoch 31\twith loss: 17.929682\t\n",
      "Finished epoch 32\twith loss: 17.985495\t\n",
      "Finished epoch 33\twith loss: 17.972534\t\n",
      "Finished epoch 34\twith loss: 17.912685\t\n",
      "Finished epoch 35\twith loss: 17.978775\t\n",
      "Finished epoch 36\twith loss: 17.986355\t\n",
      "Finished epoch 37\twith loss: 17.951902\t\n",
      "Finished epoch 38\twith loss: 17.976311\t\n",
      "Finished epoch 39\twith loss: 17.982792\t\n",
      "Finished epoch 40\twith loss: 17.969023\t\n",
      "Finished epoch 41\twith loss: 17.999109\t\n",
      "Finished epoch 42\twith loss: 17.994411\t\n",
      "Finished epoch 43\twith loss: 18.025410\t\n",
      "Finished epoch 44\twith loss: 18.050516\t\n",
      "Finished epoch 45\twith loss: 17.992493\t\n",
      "Finished epoch 46\twith loss: 17.993114\t\n",
      "Finished epoch 47\twith loss: 18.015690\t\n",
      "Finished epoch 48\twith loss: 18.000290\t\n",
      "Finished epoch 49\twith loss: 18.023619\t\n",
      "Finished epoch 50\twith loss: 18.024677\t\n",
      "Finished epoch 51\twith loss: 18.006470\t\n",
      "Finished epoch 52\twith loss: 18.049273\t\n",
      "Finished epoch 53\twith loss: 18.069349\t\n",
      "Finished epoch 54\twith loss: 18.030598\t\n",
      "Finished epoch 55\twith loss: 18.035191\t\n",
      "Finished epoch 56\twith loss: 18.063196\t\n",
      "Finished epoch 57\twith loss: 18.157957\t\n",
      "Finished epoch 58\twith loss: 18.169807\t\n",
      "Finished epoch 59\twith loss: 18.071053\t\n",
      "Finished epoch 60\twith loss: 18.091820\t\n",
      "Finished epoch 61\twith loss: 18.208334\t\n",
      "Finished epoch 62\twith loss: 18.216871\t\n",
      "Finished epoch 63\twith loss: 18.128790\t\n",
      "Finished epoch 64\twith loss: 18.202353\t\n",
      "Finished epoch 65\twith loss: 18.173763\t\n",
      "Finished epoch 66\twith loss: 18.172014\t\n",
      "Finished epoch 67\twith loss: 18.163391\t\n",
      "Finished epoch 68\twith loss: 18.065435\t\n",
      "Finished epoch 69\twith loss: 18.112213\t\n",
      "Finished epoch 70\twith loss: 18.115202\t\n",
      "Finished epoch 71\twith loss: 18.109077\t\n",
      "Finished epoch 72\twith loss: 18.063047\t\n",
      "Finished epoch 73\twith loss: 18.074183\t\n",
      "Finished epoch 74\twith loss: 18.098228\t\n",
      "Finished epoch 75\twith loss: 18.130150\t\n",
      "Finished epoch 76\twith loss: 18.096491\t\n",
      "Finished epoch 77\twith loss: 18.037386\t\n",
      "Finished epoch 78\twith loss: 18.118631\t\n",
      "Finished epoch 79\twith loss: 18.090971\t\n",
      "Finished epoch 80\twith loss: 18.115032\t\n",
      "Finished epoch 81\twith loss: 18.085732\t\n",
      "Finished epoch 82\twith loss: 18.064344\t\n",
      "Finished epoch 83\twith loss: 18.014812\t\n",
      "Finished epoch 84\twith loss: 18.047941\t\n",
      "Finished epoch 85\twith loss: 18.025379\t\n",
      "Finished epoch 86\twith loss: 18.036839\t\n",
      "Finished epoch 87\twith loss: 18.050835\t\n",
      "Finished epoch 88\twith loss: 18.010790\t\n",
      "Finished epoch 89\twith loss: 18.107935\t\n",
      "Finished epoch 90\twith loss: 18.117828\t\n",
      "Finished epoch 91\twith loss: 18.104683\t\n",
      "Finished epoch 92\twith loss: 18.094355\t\n",
      "Finished epoch 93\twith loss: 18.146681\t\n",
      "Finished epoch 94\twith loss: 18.163364\t\n",
      "Finished epoch 95\twith loss: 18.135111\t\n",
      "Finished epoch 96\twith loss: 18.158098\t\n",
      "Finished epoch 97\twith loss: 18.121525\t\n",
      "Finished epoch 98\twith loss: 18.190737\t\n",
      "Finished epoch 99\twith loss: 18.173326\t\n",
      "Finished epoch 100\twith loss: 18.192688\t\n",
      "Finished epoch 101\twith loss: 18.211361\t\n",
      "Finished epoch 102\twith loss: 18.139961\t\n",
      "Finished epoch 103\twith loss: 18.036560\t\n",
      "Finished epoch 104\twith loss: 18.133797\t\n",
      "Finished epoch 105\twith loss: 18.146318\t\n",
      "Finished epoch 106\twith loss: 18.153196\t\n",
      "Finished epoch 107\twith loss: 18.124613\t\n",
      "Finished epoch 108\twith loss: 18.103174\t\n",
      "Finished epoch 109\twith loss: 18.087666\t\n",
      "Finished epoch 110\twith loss: 18.061090\t\n",
      "Finished epoch 111\twith loss: 18.019773\t\n",
      "Finished epoch 112\twith loss: 18.025530\t\n",
      "Finished epoch 113\twith loss: 18.047037\t\n",
      "Finished epoch 114\twith loss: 18.044071\t\n",
      "Finished epoch 115\twith loss: 18.082226\t\n",
      "Finished epoch 116\twith loss: 18.067797\t\n",
      "Finished epoch 117\twith loss: 18.014004\t\n",
      "Finished epoch 118\twith loss: 18.111214\t\n",
      "Finished epoch 119\twith loss: 18.138977\t\n",
      "Finished epoch 120\twith loss: 18.105228\t\n",
      "Finished epoch 121\twith loss: 18.017626\t\n",
      "Finished epoch 122\twith loss: 18.105141\t\n",
      "Finished epoch 123\twith loss: 18.127092\t\n",
      "Finished epoch 124\twith loss: 18.164761\t\n",
      "Finished epoch 125\twith loss: 18.140776\t\n",
      "Finished epoch 126\twith loss: 18.030766\t\n",
      "Finished epoch 127\twith loss: 18.109795\t\n",
      "Finished epoch 128\twith loss: 18.191370\t\n",
      "Finished epoch 129\twith loss: 18.182581\t\n",
      "Finished epoch 130\twith loss: 18.127867\t\n",
      "Finished epoch 131\twith loss: 18.089098\t\n",
      "Finished epoch 132\twith loss: 18.042795\t\n",
      "Finished epoch 133\twith loss: 18.078438\t\n",
      "Finished epoch 134\twith loss: 18.116371\t\n",
      "Finished epoch 135\twith loss: 18.075966\t\n",
      "Finished epoch 136\twith loss: 18.060583\t\n",
      "Finished epoch 137\twith loss: 18.123611\t\n",
      "Finished epoch 138\twith loss: 18.104719\t\n",
      "Finished epoch 139\twith loss: 18.101105\t\n",
      "Finished epoch 140\twith loss: 18.083775\t\n",
      "Finished epoch 141\twith loss: 18.089964\t\n",
      "Finished epoch 142\twith loss: 18.086929\t\n",
      "Finished epoch 143\twith loss: 18.106783\t\n",
      "Finished epoch 144\twith loss: 18.064890\t\n",
      "Finished epoch 145\twith loss: 18.042095\t\n",
      "Finished epoch 146\twith loss: 18.097872\t\n",
      "Finished epoch 147\twith loss: 18.062578\t\n",
      "Finished epoch 148\twith loss: 18.082273\t\n",
      "Finished epoch 149\twith loss: 18.125753\t\n",
      "Finished training with discrete loss: 0.300499\t\n"
     ]
    }
   ],
   "source": [
    "K = 3\n",
    "lambda_vec = (N * T / 40., 20, 5 / np.sqrt(K), 10.)\n",
    "steps = 150\n",
    "\n",
    "svo = SmallVarianceOptimizer(states, dim_states, actions, dim_actions, K, lambda_vec, steps)\n",
    "svo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
